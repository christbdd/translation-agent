# 翻译代理：使用反思工作流程的代理翻译

这是一个Python展示的机器翻译的反思性代理工作流程。主要步骤是：
1. 提示一个大语言模型将文本从【源语言】翻译到【目标语言】；
2. 让大语言模型反思翻译过程，提出改进的建设性建议；
3. 使用这些建议来改进翻译。

## 可定制性 

使用大语言模型作为翻译引擎的核心，该系统具有高度的可操控性。例如，通过改变提示，使用这种工作流程比传统的机器翻译（MT）系统更容易实现以下目标：
- 修改输出的风格，如正式/非正式。
- 指定如何处理成语和特殊术语，如姓名、技术术语和缩写。例如，在提示中包含词汇表可以确保特定术语（如开源、H100或GPU）的翻译保持一致性。
- 指定特定地区的语言使用或特定方言，以服务于目标受众。例如，拉丁美洲的西班牙语与西班牙的西班牙语不同；加拿大的法语与法国的法语不同。

**这不是成熟的软件**，而是**吴恩达**在过去几个月的周末玩弄翻译的结果，加上合作者（Joaquin Dominguez、Nedelina Teneva、John Santerre）帮助重构代码。

根据我们使用传统翻译数据集上的BLEU分数进行的评估，这种工作流程有时与领先的商业产品竞争，但有时也不如它们。然而，我们也偶尔得到了极好的结果（优于商业产品），采用这种方法。我们认为这只是代理翻译的起点，并且这是一个有希望的翻译方向，有很大的改进空间，这就是为什么我们发布这个演示，以鼓励更多的讨论、实验、研究和开源贡献。

如果Agent翻译能够比传统架构（例如端到端的Transformer，它直接输入文本并输出翻译）产生更好的结果——这些架构通常运行速度更快/成本更低——这也提供了一种机制，可以自动生成训练数据（平行文本语料库），这些数据可以用于进一步训练和改进传统算法。（另见[这篇文章在The Batch](https://www.deeplearning.ai/the-batch/building-models-that-learn-from-themselves/)，关于使用大语言模型生成训练数据的内容。）

非常欢迎对如何改进这一点提出评论和建议！


## 开始使用

要开始使用 `translation-agent`，请按照以下步骤操作：

### 安装：
- 安装需要 Poetry 包管理器。[Poetry 安装](https://python-poetry.org/docs/#installation) 根据您的环境，以下命令可能有效：

```bash
pip install poetry 
```

- 需要一个包含 OPENAI_API_KEY 的 .env 文件来运行工作流。请参阅 .env.sample 文件作为示例。
```bash
git clone https://github.com/andrewyng/translation-agent.git 
cd translation-agent
poetry install
poetry shell # 激活虚拟环境
```

### 使用方法：

```python
import translation_agent as ta
source_lang, target_lang, country = "English", "Spanish", "Mexico"
translation = ta.translate(source_lang, target_lang, source_text, country)
```
请参阅 examples/example_script.py 中的示例脚本进行尝试。

## 许可证

Translation Agent 根据 **MIT 许可证** 发布。您可以自由使用、修改和分发代码，无论是商业用途还是非商业用途。

## Ideas for extensions 

Here are ideas we haven’t had time to experiment with but that we hope the open-source community will:
- **Try other LLMs.** We prototyped this primarily using gpt-4-turbo. We would love for others to experiment with other LLMs as well as other hyperparameter choices and see if some do better than others for particular language pairs. 
- **Glossary Creation.** What’s the best way to efficiently build a glossary -- perhaps using an LLM -- of the most important terms that we want translated consistently? For example, many businesses use specialized terms that are not widely used on the internet and that LLMs thus don’t know about, and there are also many terms that can be translated in multiple ways. For example, ”open source” in Spanish can be “Código abierto” or “Fuente abierta”; both are fine, but it’d better to pick one and stick with it for a single document. 
- **Glossary Usage and Implementation.** Given a glossary, what’s the best way to include it in the prompt? 
- **Evaluations on different languages.** How does its performance vary in different languages? Are there changes that make it work better for particular source or target languages? (Note that for very high levels of performance, which MT systems are approaching, we’re not sure if BLEU is a great metric.) Also, its performance on lower resource languages needs further study.  
- **Error analysis.** We’ve found that specifying a language and a country/region (e.g., “Spanish as colloquially spoken in Mexico”) does a pretty good job for our applications. Where does the current approach fall short? We’re also particularly interested in understanding its performance on specialized topics (like law, medicine) or special types of text (like movie subtitles) to understand its limitations. 
- **Better evals.** Finally, we think better evaluations (evals) is a huge and important research topic. As with other LLM applications that generate free text, current evaluation metrics appear to fall short. For example, we found that even on documents where our agentic workflow captures context and terminology better, resulting in translations that our human raters prefer over current commercial offerings, evaluation at the sentence level (using the [FLORES](https://github.com/facebookresearch/flores) dataset) resulted in the agentic system scoring lower on BLEU. Can we design better metrics (perhaps using an LLM to evaluate translations?) that capture translation quality at a document level that correlates better with human preferences? 

## Related work 

A few academic research groups are also starting to look at LLM-based and agentic translation. We think it’s early days for this field!
- *ChatGPT MT: Competitive for High- (but not Low-) Resource Languages*, Robinson et al. (2023), https://arxiv.org/pdf/2309.07423
- *How to Design Translation Prompts for ChatGPT: An Empirical Study*, Gao et al. (2023), https://arxiv.org/pdf/2304.02182v2
- *Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts*, Wu et al. (2024),  https://arxiv.org/pdf/2405.11804

